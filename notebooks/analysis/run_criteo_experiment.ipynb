{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fbfded",
   "metadata": {},
   "source": [
    "# Criteo CTR/CVR/CTCVR Experiment\n",
    "このノートブックは scripts/run_criteo_experiment.py の実行部分をインタラクティブに再現します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f612735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ROOT = Path().resolve().parents[2]  # project root\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.data.real_data import CriteoDataset, SyntheticCriteoDataset\n",
    "from src.models.esmm import ESMM\n",
    "from src.models.gbdt_proto import MTGBDT\n",
    "from src.models.stgbdt import STGBDTBaseline\n",
    "from src.models.utils import add_cvr_labels\n",
    "\n",
    "\n",
    "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    \"\"\"Compute CTR/CTCVR/CVR AUC and LogLoss.\"\"\"\n",
    "    eps = 1e-8\n",
    "    y_click = y_true[:, 0]\n",
    "    y_conv = y_true[:, 1]\n",
    "    y_ctcvr = y_click * y_conv\n",
    "\n",
    "    pred_ctr = np.clip(y_pred[:, 0], eps, 1 - eps)\n",
    "    pred_ctcvr = np.clip(y_pred[:, 1], eps, 1 - eps)\n",
    "    pred_cvr = np.clip(y_pred[:, 2], eps, 1 - eps)\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[\"auc_ctr\"] = roc_auc_score(y_click, pred_ctr)\n",
    "    metrics[\"logloss_ctr\"] = log_loss(y_click, pred_ctr)\n",
    "    metrics[\"auc_ctcvr\"] = roc_auc_score(y_ctcvr, pred_ctcvr)\n",
    "    metrics[\"logloss_ctcvr\"] = log_loss(y_ctcvr, pred_ctcvr)\n",
    "\n",
    "    click_mask = y_click == 1\n",
    "    if np.sum(click_mask) > 1 and np.unique(y_conv[click_mask]).size > 1:\n",
    "        metrics[\"auc_cvr\"] = roc_auc_score(y_conv[click_mask], pred_cvr[click_mask])\n",
    "        metrics[\"logloss_cvr\"] = log_loss(y_conv[click_mask], pred_cvr[click_mask])\n",
    "    else:\n",
    "        metrics[\"auc_cvr\"] = np.nan\n",
    "        metrics[\"logloss_cvr\"] = np.nan\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347a0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths and parameters\n",
    "sample_size = 5000\n",
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "seed = 42\n",
    "output_csv = ROOT / \"reports\" / \"tables\" / \"criteo_experiment_notebook.csv\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056933c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Criteo dataset from mt_gbm/data/criteo-research-uplift-v2.1.csv.gz...\n",
      "Error loading Criteo dataset: [Errno 2] No such file or directory: 'mt_gbm/data/criteo-research-uplift-v2.1.csv.gz'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5000, 10), (5000, 2), 'SyntheticCriteoDataset')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Criteo or Synthetic dataset\n",
    "dataset = CriteoDataset(sample_size=sample_size)\n",
    "try:\n",
    "    X, y = dataset.get_data(random_state=seed)\n",
    "    source = \"CriteoDataset\"\n",
    "except Exception:\n",
    "    synth = SyntheticCriteoDataset(sample_size=sample_size)\n",
    "    X, y = synth.load_data(random_state=seed)\n",
    "    source = \"SyntheticCriteoDataset\"\n",
    "\n",
    "X_shape, y_shape = X.shape, y.shape\n",
    "X_shape, y_shape, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=test_size + val_size, random_state=seed, stratify=y[:, 0]\n",
    ")\n",
    "rel_val = val_size / (test_size + val_size)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=1 - rel_val, random_state=seed, stratify=y_tmp[:, 0]\n",
    " )\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate ESMM\n",
    "esmm = ESMM(epochs=5, batch_size=256, verbose=0, validation_split=0.0)\n",
    "esmm.fit(X_train, y_train)\n",
    "esmm_pred = esmm.predict_proba(X_test)\n",
    "esmm_metrics = evaluate_predictions(y_test, esmm_pred)\n",
    "esmm_metrics.update({\"model\": \"ESMM\", \"n_params\": esmm.model_.count_params() if esmm.model_ else 0})\n",
    "esmm_metrics\n",
    "results.append(esmm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate MTGBDT\n",
    "mtgbdt = MTGBDT(\n",
    "    n_estimators=20, learning_rate=0.1, max_depth=3, n_tasks=3, loss=\"logloss\", weighting_strategy=\"mtgbm\"\n",
    " )\n",
    "mtgbdt.fit(X_train, y_train)\n",
    "mtgbdt_pred = mtgbdt.predict_proba(X_test)\n",
    "mtgbdt_metrics = evaluate_predictions(y_test, mtgbdt_pred)\n",
    "mtgbdt_metrics.update({\"model\": \"MTGBDT\", \"n_estimators\": mtgbdt.n_estimators})\n",
    "mtgbdt_metrics\n",
    "results.append(mtgbdt_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ddf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate STGBDT baseline\n",
    "stg = STGBDTBaseline(\n",
    "    n_estimators=10, learning_rate=0.3, max_depth=2, min_samples_split=20, min_samples_leaf=10\n",
    " )\n",
    "stg.fit(X_train, y_train)\n",
    "stg_pred = stg.predict_proba(X_test)\n",
    "stg_metrics = evaluate_predictions(y_test, stg_pred)\n",
    "stg_metrics.update({\"model\": \"STGBDTBaseline\", \"n_estimators\": stg.n_estimators})\n",
    "stg_metrics\n",
    "results.append(stg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and display metrics\n",
    "headers = [\"model\", \"auc_ctr\", \"auc_ctcvr\", \"auc_cvr\", \"logloss_ctr\", \"logloss_ctcvr\", \"logloss_cvr\"]\n",
    "print(\"\\n=== Experiment Results ===\")\n",
    "for res in results:\n",
    "    row = {h: res.get(h, np.nan) for h in headers}\n",
    "    print(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f501ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to CSV\n",
    "output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_results.to_csv(output_csv, index=False)\n",
    "output_csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-gbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
